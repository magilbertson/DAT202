{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Module 5 - Clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Topic 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### To start, we are going to step through a standard example.  We will first create 4 sets of random numbers in clusters and then cluster them (which should obviously give great results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import statements\n",
    "from sklearn.datasets import make_blobs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# create blobs\n",
    "data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.6, random_state=50)\n",
    "# print(data[0])  We want the first array in this array because it has the datapoints.  The other array in this array is the clusters of the data\n",
    "# create np array for data points\n",
    "points = data[0]\n",
    "print(points[0:5])\n",
    "print(points.shape)\n",
    "print(type(points))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's see the data on a scatter plot first to get an idea of what we are looking at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create scatter plot\n",
    "plt.scatter(data[0][:,0], data[0][:,1]) # data[0][:,1] is the first data column in the first array\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "plt.show()\n",
    "plt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis') #Adding cluster coloring from the creation of the random clusters\n",
    "plt.xlim(-15,15)\n",
    "plt.ylim(-15,15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now to build a k-means model to assign these to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import KMeans\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create kmeans object\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "print(\"data type:\\n\",type(kmeans),end='\\n\\n')\n",
    "# fit kmeans object to data\n",
    "kmeans.fit(points)\n",
    "# print location of clusters learned by kmeans object\n",
    "print(\"centers:\\n\",kmeans.cluster_centers_, end='\\n\\n')\n",
    "# save new clusters for chart\n",
    "y_km = kmeans.fit_predict(points)\n",
    "print(\"assigned cluster:\\n\",y_km,end='\\n\\n')\n",
    "print(\"cluster prediction variable type:\\n\",type(y_km),end=\"\\n\\n\")\n",
    "print(\"cluster prediction variable shape:\\n\",y_km.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#print(y_km==0) #Shows the result of the comparison operator on y_km\n",
    "#print(points[y_km ==0,0]) #Shows the result of y_comparison and the data point from points\n",
    "plt.scatter(points[y_km ==0,0], points[y_km == 0,1], s=100, c='red')\n",
    "plt.scatter(points[y_km ==1,0], points[y_km == 1,1], s=100, c='green')\n",
    "plt.scatter(points[y_km ==2,0], points[y_km == 2,1], s=100, c='blue')\n",
    "plt.scatter(points[y_km ==3,0], points[y_km == 3,1], s=100, c='cyan')\n",
    "# This part adds the discovered cluster centers to the plot\n",
    "for x_pt,y_pt in kmeans.cluster_centers_:\n",
    "    #print(x_pt,y_pt)\n",
    "    plt.scatter(x_pt,y_pt, color='black',s=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's see how this same data looks with hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import hierarchical clustering libraries\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with plt.rc_context(): # This lets us change the size of just this visual without affecting the rest of the notebook\n",
    "    plt.rc('figure',figsize=(28,10))\n",
    "    # create dendrogram\n",
    "    dendrogram = sch.dendrogram(sch.linkage(points, method='ward'))\n",
    "    # create clusters\n",
    "    hc = AgglomerativeClustering(n_clusters=4, affinity = 'euclidean', linkage = 'ward')\n",
    "    # save clusters for chart\n",
    "    y_hc = hc.fit_predict(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's plot the points with their assigned clusters again for hierarchical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(points[y_hc ==0,0], points[y_hc == 0,1], s=100, c='red')\n",
    "plt.scatter(points[y_hc==1,0], points[y_hc == 1,1], s=100, c='black')\n",
    "plt.scatter(points[y_hc ==2,0], points[y_hc == 2,1], s=100, c='blue')\n",
    "plt.scatter(points[y_hc ==3,0], points[y_hc == 3,1], s=100, c='cyan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now that we've seen the pretty clustering with faked up data, let's give this a try with something real.  Last week we worked with the interests dataset and I want to see how it clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "interest_data = pd.read_csv('responses.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### For the first part, let's look specifically at the music data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "music_type_interest = interest_data.iloc[:,1:18].to_numpy()\n",
    "# music_type_interest_pd = interest_data.iloc[:,1:18] # Could also do pandas dataframe but code below was easy in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(music_type_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now let's cluster it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Note:this fails on purpose!\n",
    "# create kmeans object\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "# fit kmeans object to data\n",
    "kmeans.fit(music_type_interest_pd)\n",
    "# print location of clusters learned by kmeans object\n",
    "print(kmeans.cluster_centers_)\n",
    "# save new clusters for chart\n",
    "y_km = kmeans.fit_predict(music_type_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What happened?  I made a mistake by not prepping my data properly.  It turns out that there are several missing values in these columns of data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Recall from DAT201 that we have to decide how we handle missing values.  We could delete the rows missing data, fill with 1's or 0's, or use an average of the rest of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First, let's see how many missing pieces of data we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(music_type_interest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now, let's look at one of those rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(music_type_interest[8,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### I've decided to replace all of the nan values with the average for that column because this should have the least affect on my clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### First, we'll figure out the average for each of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "col_mean = np.nanmean(music_type_interest, axis=0)\n",
    "print(\"column average ignoring NaN:\\n\",col_mean, end=\"\\n\\n\")\n",
    "nans_to_replace = np.where(np.isnan(music_type_interest)) #tuple of arrays for row/column pairs that need to be updated\n",
    "print(\"datatype of nans_to_replace:\\n\",type(nans_to_replace), end = \"\\n\\n\")\n",
    "print(\"tuple of arrays for NaN's to replace:\\n\",nans_to_replace, end=\"\\n\\n\") #This is a tuple of two arrays.  The first is the row with the NaNs and the second is the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "music_type_interest[nans_to_replace] = np.take(col_mean,nans_to_replace[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(music_type_interest[8,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Now let's try our kmeans again now that the data is cleaned up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# create kmeans object\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "# fit kmeans object to data\n",
    "kmeans.fit(music_type_interest)\n",
    "# print location of clusters learned by kmeans object\n",
    "print(kmeans.cluster_centers_)\n",
    "# save new clusters for chart\n",
    "y_km = kmeans.fit_predict(music_type_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### That's a lot of numbers.  We need to look at our best metric to see if these clusters are actually valid/worthwhile...Silhouette score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "score = silhouette_score(music_type_interest, y_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### It doesn't look like there's very good clustering here.  However, to be sure, I'd like to run a range of cluster counts to see the highest silhouette coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sse = {}\n",
    "for n_clusters in range(2,12):\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    preds = clusterer.fit_predict(music_type_interest)\n",
    "    centers = clusterer.cluster_centers_\n",
    "    sse[n_clusters] = clusterer.inertia_\n",
    "    score = silhouette_score(music_type_interest, preds)\n",
    "    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))\n",
    "print(sse)\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We'll be doing some visualization to see things more clearly next week.  However, I want to have a better result so I'm going to cheat a bit.  From the dataset, I'm going to grab the columns that seem the most likely to be different (this is an example of inserting my own bias...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's use the columns Folk, Raggae/Ska, and Latino.  I doubt that everybody has the same level of interest between these three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "folk_raggae_latino_interests = music_type_interest[:,[2,11,15]]\n",
    "print(music_type_interest[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(folk_raggae_latino_interests[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sse = {}\n",
    "for n_clusters in range(2,12):\n",
    "    clusterer = KMeans(n_clusters=n_clusters)\n",
    "    preds = clusterer.fit_predict(folk_raggae_latino_interests)\n",
    "    centers = clusterer.cluster_centers_\n",
    "    sse[n_clusters] = clusterer.inertia_\n",
    "    score = silhouette_score(folk_raggae_latino_interests, preds)\n",
    "    print(\"For n_clusters = {}, silhouette score is {})\".format(n_clusters, score))\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of cluster\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A bit better but still not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's try it with hierachical clustering and see if we get a different result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import hierarchical clustering libraries (again)\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with plt.rc_context():\n",
    "    plt.rc('figure',figsize=(28,28))# create dendrogram\n",
    "    dendrogram = sch.dendrogram(sch.linkage(folk_raggae_latino_interests, method='ward'))\n",
    "    # create clusters\n",
    "    hc = AgglomerativeClustering(n_clusters=6, compute_full_tree=False, affinity = 'euclidean', linkage = 'ward')#, distance_threshold = 40)\n",
    "    print(hc)\n",
    "    # save clusters for chart\n",
    "    y_hc = hc.fit_predict(folk_raggae_latino_interests)\n",
    "    score_hier = silhouette_score(folk_raggae_latino_interests, y_hc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(score_hier)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
